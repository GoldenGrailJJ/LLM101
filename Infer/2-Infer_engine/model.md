## 1. Batch Normalization (BN)

Batch Normalization 的目标是使每一层的输入分布稳定。在序列任务中，BN 会对每个时间步的位置进行标准化，考虑批次中的所有样本。

### 计算过程：
假设输入数据为 $ X \in \mathbb{R}^{m \times L \times d} $，其中：

- $ m $ 是批次大小，
- $ L $ 是序列长度，
- $ d $ 是每个序列单元的特征维度。

1. **计算均值和方差：**

   对每个时间步 $ i $ 和特征维度 $ j $ 计算批次中的均值和方差：
   $$
   \mu_{i,j} = \frac{1}{m} \sum_{k=1}^{m} X_{k,i,j}
   $$
   $$
   \sigma_{i,j}^2 = \frac{1}{m} \sum_{k=1}^{m} (X_{k,i,j} - \mu_{i,j})^2
   $$

2. **标准化：**

   对每个样本的每个特征进行标准化：
   $$
   \hat{X}_{k,i,j} = \frac{X_{k,i,j} - \mu_{i,j}}{\sqrt{\sigma_{i,j}^2 + \epsilon}}
   $$

3. **线性变换：**

   最后进行线性变换：
   $$
   Y_{k,i,j} = \gamma_j \hat{X}_{k,i,j} + \beta_j
   $$

---

## 2. Layer Normalization (LN)

Layer Normalization 是对每个样本的所有特征进行标准化，而不是依赖于批次大小。它适用于序列数据中的每个时间步。

### 计算过程：
假设输入数据为 $ X \in \mathbb{R}^{m \times L \times d} $，其中：

- $ m $ 是批次大小，
- $ L $ 是序列长度，
- $ d $ 是每个序列单元的特征维度。

1. **计算均值和方差：**

   对每个样本 $ x_k $ 计算均值和方差：
   $$
   \mu_k = \frac{1}{d} \sum_{j=1}^{d} x_{k,j}
   $$
   $$
   \sigma_k^2 = \frac{1}{d} \sum_{j=1}^{d} (x_{k,j} - \mu_k)^2
   $$

2. **标准化：**

   对样本 $ x_k $ 中的每个特征进行标准化：
   $$
   \hat{x}_{k,j} = \frac{x_{k,j} - \mu_k}{\sqrt{\sigma_k^2 + \epsilon}}
   $$

3. **线性变换：**

   最后进行线性变换：
   $$
   y_{k,j} = \gamma_j \hat{x}_{k,j} + \beta_j
   $$

---

## 3. RMSNorm (Root Mean Square Normalization)

RMSNorm 是 Layer Normalization 的变体，使用均方根（RMS）代替均值和方差进行标准化，简化了计算过程。

### 计算过程：
假设输入数据为 $ X \in \mathbb{R}^{m \times L \times d} $，其中：

- $ m $ 是批次大小，
- $ L $ 是序列长度，
- $ d/j $ 是每个序列单元的特征维度。

1. **计算均方根（RMS）：**

   对每个样本 $ x_k $ 计算均方根（RMS）：
   $$
   \text{RMS}(x_k) = \sqrt{\frac{1}{d} \sum_{j=1}^{d} x_{k,j}^2}
   $$

2. **标准化：**

   对样本 $ x_k $ 中的每个特征进行标准化：
   $$
   \hat{x}_{k,j} = \frac{x_{k,j}}{\text{RMS}(x_k) + \epsilon}
   $$

3. **线性变换：**

   最后进行线性变换：
   $$
   y_{k,j} = \gamma_j \hat{x}_{k,j} + \beta_j
   $$

---

## 比较总结

- **Batch Normalization (BN)**：对批次中的每个时间步位置进行标准化，依赖批次的均值和方差。
- **Layer Normalization (LN)**：对每个样本的所有特征进行标准化，不依赖批次大小，适合序列数据。
- **RMSNorm**：与 LN 相似，但使用均方根（RMS）代替均值和方差进行标准化，计算更简洁。

以下是所有公式中符号两端都加上 $ 的 Markdown 格式：

# 正弦-余弦位置编码（Sinusoidal Positional Encoding）

## 背景与目的
Transformer 模型无法直接感知输入序列的顺序，因此需要位置编码（Positional Encoding）为序列中的每个位置提供位置信息。正弦-余弦位置编码是一种基于固定公式的方法，常用于 Transformer 模型中。

---

## 正弦-余弦编码公式
对于序列中的某个位置 $pos$ 和编码维度 $i$：

1. **偶数维度（索引为 $2i$）：**
   $$
   PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
   $$

2. **奇数维度（索引为 $2i+1$）：**
   $$
   PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)
   $$

其中：
- $pos$ 是序列的位置索引（如第 1 个单词的位置为 0，第 2 个单词的位置为 1，以此类推）。
- $i$ 是当前编码维度的索引。
- $d$ 是位置编码的总维度（如 4、8 等）。
- $10000$ 是一个常数，用于控制频率范围。

---

## 示例：对一句话进行编码

### 输入
- **句子**：`"I love AI."`
- **词嵌入维度**：$d = 4$
  - `I` 的嵌入向量：$[1.0, 0.5, -1.0, 0.3]$
  - `love` 的嵌入向量：$[0.8, -0.2, 0.3, 0.7]$
  - `AI` 的嵌入向量：$[0.1, 0.9, -0.4, 0.5]$

### 正弦-余弦位置编码计算
#### 1. 对于位置 $pos = 0$：
$$
PE(0, 0) = \sin\left(\frac{0}{10000^{0/4}}\right) = \sin(0) = 0
$$
$$
PE(0, 1) = \cos\left(\frac{0}{10000^{0/4}}\right) = \cos(0) = 1
$$
$$
PE(0, 2) = \sin\left(\frac{0}{10000^{2/4}}\right) = \sin(0) = 0
$$
$$
PE(0, 3) = \cos\left(\frac{0}{10000^{2/4}}\right) = \cos(0) = 1
$$

编码为：$[0, 1, 0, 1]$

#### 2. 对于位置 $pos = 1$：
$$
PE(1, 0) = \sin\left(\frac{1}{10000^{0/4}}\right) = \sin(1) \approx 0.841
$$
$$
PE(1, 1) = \cos\left(\frac{1}{10000^{0/4}}\right) = \cos(1) \approx 0.540
$$
$$
PE(1, 2) = \sin\left(\frac{1}{10000^{2/4}}\right) = \sin(0.01) \approx 0.010
$$
$$
PE(1, 3) = \cos\left(\frac{1}{10000^{2/4}}\right) = \cos(0.01) \approx 0.999
$$

编码为：$[0.841, 0.540, 0.010, 0.999]$

#### 3. 对于位置 $pos = 2$：
$$
PE(2, 0) = \sin\left(\frac{2}{10000^{0/4}}\right) = \sin(2) \approx 0.909
$$
$$
PE(2, 1) = \cos\left(\frac{2}{10000^{0/4}}\right) = \cos(2) \approx -0.416
$$
$$
PE(2, 2) = \sin\left(\frac{2}{10000^{2/4}}\right) = \sin(0.02) \approx 0.020
$$
$$
PE(2, 3) = \cos\left(\frac{2}{10000^{2/4}}\right) = \cos(0.02) \approx 0.999
$$

编码为：$[0.909, -0.416, 0.020, 0.999]$

---

### 最终嵌入表示
将词嵌入与位置编码逐元素相加，得到最终的嵌入表示：

1. **位置 $pos = 0$（单词 `I`）：**
   $$
   [1.0, 0.5, -1.0, 0.3] + [0, 1, 0, 1] = [1.0, 1.5, -1.0, 1.3]
   $$

2. **位置 $pos = 1$（单词 `love`）：**
   $$
   [0.8, -0.2, 0.3, 0.7] + [0.841, 0.540, 0.010, 0.999] = [1.641, 0.340, 0.310, 1.699]
   $$

3. **位置 $pos = 2$（单词 `AI`）：**
   $$
   [0.1, 0.9, -0.4, 0.5] + [0.909, -0.416, 0.020, 0.999] = [1.009, 0.484, -0.380, 1.499]
   $$

---

### 结果总结
- **原始词嵌入**：
  - `I`：$[1.0, 0.5, -1.0, 0.3]$
  - `love`：$[0.8, -0.2, 0.3, 0.7]$
  - `AI`：$[0.1, 0.9, -0.4, 0.5]$

- **正弦-余弦位置编码**：
  - 位置 $pos = 0$：$[0, 1, 0, 1]$
  - 位置 $pos = 1$：$[0.841, 0.540, 0.010, 0.999]$
  - 位置 $pos = 2$：$[0.909, -0.416, 0.020, 0.999]$

- **最终嵌入表示**：
  - `I`：$[1.0, 1.5, -1.0, 1.3]$
  - `love`：$[1.641, 0.340, 0.310, 1.699]$
  - `AI`：$[1.009, 0.484, -0.380, 1.499]$

---

## 正弦-余弦编码的优点
1. **无需学习参数**：编码是静态计算的，不需要模型学习。
2. **包含相对位置信息**：通过正弦和余弦的周期性变化，自然捕获相对位置关系。对于固定的维度 $i$, 随着$pos$的增加，编码的值呈现周期性变化，模型能够区分输入序列中token的不同位置。对于固定$pos$随着$i$增加频率逐渐降低，对位置变化更加不敏感。
3. **对长序列的扩展性强**：位置编码适用于任意长度的序列，且具备远程衰减能力。两个向量之间的内积大小会随着它们位置之间的差值的增大而减小。

## 可能的局限性
1. **对任务的不灵活性**：固定编码可能无法适应特定任务的数据分布。此外，由于编码是作用在向量间还没有交互的阶段，所以无法引入相对位置信息。这就导致再经过attention计算后，起作用的交互项部分不再具备绝对、相对、外推性。
2. **长序列的数值精度问题**：对于非常长的序列，数值可能出现不稳定。浮点精度会影响编码结果。

## 可视化
正弦和余弦的位置编码在不同维度上的值会呈现周期性波动，低频对应长距离信息，高频对应短距离信息。通过这些特征，模型能够理解序列中不同单词的位置信息及其关系。

# RoPE 位置编码解决了正弦-余弦编码的哪些问题？

## 背景
正弦-余弦位置编码（Sinusoidal Positional Encoding）是一种经典的位置编码方法，用于在 Transformer 模型中引入位置信息。然而，正弦-余弦编码存在以下局限性，尤其在长序列和相对位置信息的建模中效果不足。

RoPE（Rotary Position Embedding，旋转位置编码）通过对查询向量（Query）和键向量（Key）进行旋转调整，巧妙地解决了这些问题，同时保持高效性和实现的简单性。

---

## RoPE 的公式及核心思想

### 1. **RoPE 的基本公式**

在 RoPE 中，查询向量 $Q$ 和键向量 $K$ 被嵌入旋转位置编码，公式如下：

- **对于查询向量 $Q$ 和键向量 $K$**：
  $$
  Q_{\text{rotated}} = \text{RoPE}(Q) = Q_r \cos(\theta) - Q_i \sin(\theta) + i \cdot (Q_r \sin(\theta) + Q_i \cos(\theta))
  $$
  $$
  K_{\text{rotated}} = \text{RoPE}(K) = K_r \cos(\theta) - K_i \sin(\theta) + i \cdot (K_r \sin(\theta) + K_i \cos(\theta))
  $$

  其中：
  - $Q_r$ 和 $K_r$ 分别为 $Q$ 和 $K$ 的实部（偶数维分量）。
  - $Q_i$ 和 $K_i$ 分别为 $Q$ 和 $K$ 的虚部（奇数维分量）。
  - $\theta$ 是通过正弦-余弦位置编码生成的旋转角度：
    $$
    \theta = \frac{pos}{10000^{\frac{2i}{d}}}
    $$
    其中 $pos$ 为位置，$d$ 为维度。

- **点积计算公式**：
  在嵌入 RoPE 旋转位置编码后，注意力机制中的点积自然包含了位置关系：
  $$
  (Q_{\text{rotated}})(K_{\text{rotated}})^T = QK^T + \text{相对位置信息}
  $$

---

### 2. **旋转操作的几何意义**

- RoPE 的核心是将位置编码转化为复数旋转操作：
  - 偶数维度作为实部，奇数维度作为虚部。
  - 通过旋转角度 $\theta$ 实现位置编码，角度变化直接反映在注意力计算中。
  
- **复数旋转公式的几何解释**：
  对于任意向量 $(x_r, x_i)$，经过旋转后变为：
  $$
  x'_r = x_r \cos(\theta) - x_i \sin(\theta)
  $$
  $$
  x'_i = x_r \sin(\theta) + x_i \cos(\theta)
  $$

---

## RoPE 解决的正弦-余弦编码问题

### 1. **缺乏对相对位置信息的直接建模**
- **问题**：
  正弦-余弦编码主要捕获**绝对位置信息**，无法直接表达序列中两个位置之间的相对关系。这导致在长序列任务中，模型难以捕获单词之间的相对位置依赖。

- **RoPE 的解决方法**：
  RoPE 将正弦-余弦编码转化为旋转操作，通过调整查询向量 $Q$ 和键向量 $K$ 的相对旋转，隐式嵌入相对位置信息。这使得在点积计算时，能够自然体现位置差异。

---

### 2. **对长序列的支持能力不足**
- **问题**：
  正弦-余弦编码在长序列中可能面临数值精度问题：
  - 随着位置增加，高频部分的正弦和余弦值可能趋于零，导致位置信息的表达失效。
  - 需要预定义序列的最大长度，超出范围时无法直接使用。

- **RoPE 的解决方法**：
  - **精度问题**：RoPE 使用旋转矩阵调整 $Q$ 和 $K$，旋转操作的数值稳定性较高，对长序列的位置信息依然敏感。
  - **扩展性**：RoPE 不依赖于预定义的最大序列长度，可以灵活适应更长的序列任务。这里通过$base = 10000$来控制转动的快慢，如果这个值很大，那么圆盘转动的慢

---

### 3. **不支持与注意力机制的自然结合**
- **问题**：
  正弦-余弦编码需要通过加法方式将位置编码添加到嵌入中，无法直接作用于注意力机制。这种方式虽然简单，但可能对序列间的依赖关系建模不够直观。

- **RoPE 的解决方法**：
  RoPE 将位置编码直接嵌入到注意力机制中：
  - 通过对 $Q$ 和 $K$ 的旋转调整，使得在点积计算 $QK^T$ 时，已隐式包含位置关系。
  - 这一设计简化了位置编码与注意力机制的结合，同时提高了模型捕捉位置关系的能力。

---

### 4. **额外计算成本**
- **问题**：
  一些改进的相对位置编码方法（如 Transformer-XL）通常需要显式引入相对位置偏置或额外参数，增加了模型的复杂性和计算开销。

- **RoPE 的解决方法**：
  RoPE 不需要引入额外的参数或复杂的计算，仅通过对 $Q$ 和 $K$ 的旋转调整实现。这使得 RoPE 的实现非常高效，且计算复杂度与传统正弦-余弦编码相当。

---

### 5. **特定任务表现不足**
- **问题**：
  在涉及相对位置信息的任务中（如长文本生成、翻译、问答等），正弦-余弦编码无法充分捕获上下文间的相对位置信息，限制了模型性能。

- **RoPE 的解决方法**：
  - RoPE 在捕获相对位置信息时表现出色，使得模型能够更准确地处理长序列任务。
  - 实验证明，使用 RoPE 的模型（如 LLaMA）在长文本生成和其他长序列任务中表现优异。

---

## 总结

RoPE 通过将位置编码转化为旋转操作，解决了正弦-余弦编码的以下问题：
1. **缺乏相对位置信息建模能力**：RoPE 自然嵌入相对位置信息，增强了模型的上下文捕获能力。
2. **长序列支持能力不足**：RoPE 对长序列更敏感且数值稳定性更高。
3. **不支持直接与注意力机制结合**：RoPE 将旋转操作嵌入到 $Q$ 和 $K$ 的点积中，自然结合注意力机制。
4. **额外计算开销高**：RoPE 保持了高效的计算性能，不引入额外参数。
5. **特定任务表现不足**：RoPE 在长序列任务和相对位置依赖的任务中性能优异。

因此，RoPE 成为现代 Transformer 模型中重要的位置编码方法之一，并广泛应用于 LLaMA、GPT-NeoX 等模型中。





