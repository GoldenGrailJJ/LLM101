# Import all necessary helper functions and libraries from Lesson 1 and 2
import helpers
from helpers import init_batch, generate_next_token
from helpers import merge_batches, filter_batch
import copy
import matplotlib.pyplot as plt
import numpy as np
import random
import time
import torch
import torch.nn.functional as F
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer



def init_batch(requests):
    """
    Initializes a batch from the list of requests. Each request contains a prompt and 
    the number of tokens remaining to be generated for that prompt.

    Args:
    - requests (list of tuple): List of tuples where each tuple contains:
        - prompt (str): The input text.
        - tokens_remaining (int): The number of tokens still to be generated for this prompt.

    Returns:
    - dict: A dictionary containing:
        - "position_ids": Tensor of position ids for each token in the prompt.
        - "responses": Copy of the input prompts.
        - "tokens_remaining": List of the remaining tokens for each request.
        - Tokenized input tensors: 'input_ids' and 'attention_mask'.
    """
    # Extract prompts from the requests
    prompts = [r[0] for r in requests]
    
    # Tokenize the prompts and pad them to the same length
    inputs = tokenizer(prompts, padding=True, return_tensors="pt")
    
    # Create position IDs based on the attention mask (used to track token positions)
    attention_mask = inputs["attention_mask"]
    position_ids = attention_mask.long().cumsum(-1) - 1  # Cumulative sum for position tracking
    position_ids.masked_fill_(attention_mask == 0, 1)  # Fill padding positions with 1
    
    # Return a dictionary with tokenized inputs and additional information
    return {
        "position_ids": position_ids,
        "responses": copy.copy(prompts),  # Store the original prompts
        "tokens_remaining": [r[1] for r in requests],  # Track how many tokens remain for each prompt
        **inputs  # Add tokenized 'input_ids' and 'attention_mask'
    }

def generate_next_token(batch):
    """
    Generates the next token(s) for each prompt in the batch using the model.
    
    Args:
    - batch (dict): A batch of tokenized inputs. It must contain:
        - "input_ids": The tokenized inputs.
        - "attention_mask": The attention mask for the batch.
        - "position_ids": Position IDs.
        - "past_key_values": The previous key-value pairs used for efficient generation.
    
    Returns:
    - dict: The updated batch containing:
        - "input_ids": New tokens generated by the model.
        - "past_key_values": Updated key-value pairs for caching.
        - "responses": Updated responses.
        - "tokens_remaining": Updated tokens remaining for each prompt.
    """
    # Create a copy of the batch and remove unnecessary keys
    inputs = copy.copy(batch)
    inputs.pop("responses")  # Remove responses for processing
    inputs.pop("tokens_remaining")  # Remove tokens_remaining
    
    # Generate the next token(s) and get updated past key-values
    next_token_ids, past_key_values = generate_batch_tokens_with_past(inputs)
    
    # Decode the generated token IDs back into text
    next_tokens = tokenizer.batch_decode(next_token_ids)
    
    # Return the updated inputs with the newly generated tokens
    return get_next_inputs(batch, next_token_ids, past_key_values, next_tokens)
def merge_batches(batch1, batch2):
    """
    Merges two batches into one. It ensures that both batches are padded to the same length 
    for consistency in subsequent operations.

    Args:
    - batch1 (dict): First batch to merge.
    - batch2 (dict): Second batch to merge.

    Returns:
    - dict: The merged batch containing:
        - "input_ids": Concatenated input IDs from both batches.
        - "position_ids": Concatenated position IDs from both batches.
        - "attention_mask": Merged attention mask.
        - "past_key_values": Merged past key-value pairs.
        - "responses": Concatenated responses from both batches.
        - "tokens_remaining": Concatenated list of remaining tokens for each prompt.
    """
    # Find the max sequence length from both batches
    attn_mask1 = batch1["attention_mask"]
    attn_mask2 = batch2["attention_mask"]
    max_seq_len = max(attn_mask1.shape[1], attn_mask2.shape[1])
    
    # Pad the attention masks to the same length (on the left)
    padding1 = max_seq_len - attn_mask1.shape[1]
    padding2 = max_seq_len - attn_mask2.shape[1]
    attn_mask1 = F.pad(attn_mask1, (padding1, 0), "constant", 0)
    attn_mask2 = F.pad(attn_mask2, (padding2, 0), "constant", 0)
    
    # Pad past_key_values
    past_kv1 = batch1["past_key_values"]
    past_kv2 = batch2["past_key_values"]
    
    padded_kv1 = []
    for i in range(len(past_kv1)):
        k, v = past_kv1[i]
        k = F.pad(k, (0, 0, padding1, 0), "constant", 0)
        v = F.pad(v, (0, 0, padding1, 0), "constant", 0)     
        padded_kv1.append((k, v))
    
    padded_kv2 = []
    for i in range(len(past_kv2)):
        k, v = past_kv2[i]
        k = F.pad(k, (0, 0, padding2, 0), "constant", 0)
        v = F.pad(v, (0, 0, padding2, 0), "constant", 0)     
        padded_kv2.append((k, v))
    
    # Concatenate input tensors
    input_ids = torch.concat([batch1["input_ids"], batch2["input_ids"]], dim=0)
    position_ids = torch.concat([batch1["position_ids"], batch2["position_ids"]], dim=0) 
    attn_mask = torch.concat([attn_mask1, attn_mask2], dim=0)
    
    # Concatenate past key-values
    past_kv = []
    for i in range(len(padded_kv1)):
        k1, v1 = padded_kv1[i]
        k2, v2 = padded_kv2[i]
        k = torch.concat([k1, k2], dim=0)
        v = torch.concat([v1, v2], dim=0)
        past_kv.append((k, v))
    
    # Return the merged batch
    return {
        "input_ids": input_ids,
        "position_ids": position_ids,
        "attention_mask": attn_mask,
        "past_key_values": past_kv,
        "responses": batch1["responses"] + batch2["responses"],
        "tokens_remaining": batch1["tokens_remaining"] + batch2["tokens_remaining"],
    }
def filter_batch(batch):
    # Step 1: Identify the rows where the number of remaining tokens is 0
    # These rows will be removed from the batch
    remove_indices = []
    for i, tokens_remaining in enumerate(batch["tokens_remaining"]):
        if tokens_remaining <= 0:
            # If no tokens are remaining, mark the index for removal
            remove_indices.append(i)
    
    # Step 2: Create a mask that indicates which rows to keep (not to be removed)
    batch_size = batch["input_ids"].size(0)
    mask = torch.ones(batch_size, dtype=torch.bool)
    # Mark the rows to be removed as False in the mask
    mask[remove_indices] = False

    # Step 3: Use the mask to filter out rows from each tensor in the batch
    # The mask is used to select the rows to keep (i.e., rows with remaining tokens > 0)
    input_ids = batch["input_ids"][mask]
    position_ids = batch["position_ids"][mask]
    attention_mask = batch["attention_mask"][mask]
    
    # Filter the 'responses' list, keeping only the entries corresponding to non-removed rows
    responses = [
        r
        for i, r in enumerate(batch["responses"])
        if i not in remove_indices
    ]
    
    # Similarly, filter the 'tokens_remaining' list
    tokens_remaining = [
        v
        for i, v in enumerate(batch["tokens_remaining"])
        if i not in remove_indices
    ]

    # Step 4: Filter the 'past_key_values' list, keeping only the rows that are not removed
    past_key_values = batch["past_key_values"]
    new_past_key_values = []
    for i in range(len(past_key_values)):
        k, v = past_key_values[i]
        # Apply the mask to both keys (k) and values (v) in the past_key_values
        k = k[mask]
        v = v[mask]
        new_past_key_values.append((k, v))
    past_key_values = new_past_key_values
    
    # Step 5: Optimize further by truncating the attention_mask and past_key_values
    # Truncate the attention mask and past_key_values to the longest remaining sequence length
    if input_ids.size(0) > 0:
        # Create a mask to identify the positions of padding tokens (zeros in the attention mask)
        zero_mask = attention_mask == 0
        # Calculate the cumulative product of the zero_mask, which helps find the first non-zero index
        cumprod = zero_mask.cumprod(dim=1)  
        # Count the number of leading zeros for each row
        leading_zeros_count = cumprod.sum(dim=1)
        # Find the minimum count of leading zeros across all rows
        min_leading_zeros = torch.min(leading_zeros_count)
        # Determine the offset where the truncation should occur (i.e., the position of the first non-zero token)
        truncation_offset = min_leading_zeros.item()

        # Step 6: Apply truncation to the attention_mask and past_key_values based on the computed offset
        attention_mask = attention_mask[:, truncation_offset:]
        
        # Apply the same truncation to the past_key_values
        new_past_key_values = []
        for i in range(len(past_key_values)):
            k, v = past_key_values[i]
            # Truncate the key and value tensors along the sequence dimension
            k = k[:, :, truncation_offset:, :]
            v = v[:, :, truncation_offset:, :]
            new_past_key_values.append((k, v))
        past_key_values = new_past_key_values
    
    # Step 7: Return the filtered batch along with the indices of the removed rows
    return {
        "input_ids": input_ids,
        "position_ids": position_ids,
        "attention_mask": attention_mask,
        "past_key_values": past_key_values,
        "responses": responses,
        "tokens_remaining": tokens_remaining,
    }, remove_indices


# Load the pre-trained GPT-2 model and tokenizer
model_name = "./models/gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Define PAD Token as the same as EOS Token (50256 in GPT-2)
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# Set padding to occur on the left side, so new tokens are appended on the right
tokenizer.padding_side = "left"
tokenizer.truncation_side = "left"

# Define a list of prompts of varying lengths to send to the model in a batch
prompts = [
    "The quick brown fox jumped over the",
    "The rain in Spain falls",
    "What comes up must",
]

# Tokenize the prompts and ensure padding is added to make the sequences the same length
# `return_tensors="pt"` ensures that the output is in PyTorch tensor format
inputs = tokenizer(prompts, padding=True, return_tensors="pt")

# Define the function to generate tokens for a batch using cached past key-values
def generate_batch_tokens_with_past(inputs):
    with torch.no_grad():
        # Perform inference (forward pass) with no gradient calculation
        outputs = model(**inputs)

    # Get the logits (output probabilities) from the model's final layer
    logits = outputs.logits
    # Extract the logits for the last token in the sequence (last position of each input)
    last_logits = logits[:, -1, :]
    # Get the token with the highest probability from the last logits (the next token prediction)
    next_token_ids = last_logits.argmax(dim=1)
    return next_token_ids, outputs.past_key_values

# Define the function to generate tokens for a batch, handling past key-values for caching
def generate_batch(inputs, max_tokens):
    # Create an empty list to store generated tokens for each prompt in the batch
    generated_tokens = [[] for _ in range(inputs["input_ids"].shape[0])]
    
    # Compute position IDs based on the attention mask
    attention_mask = inputs["attention_mask"]
    position_ids = attention_mask.long().cumsum(-1) - 1  # Calculate token positions
    position_ids.masked_fill_(attention_mask == 0, 1)  # Set position of padding tokens to 1
    
    # Prepare the inputs for the next token generation with position_ids included
    next_inputs = {
        "position_ids": position_ids,
        **inputs
    }

    # Generate tokens iteratively for max_tokens iterations
    for _ in range(max_tokens):
        # Call the function to generate the next token and get the cached past key-values
        next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)
        
        # Prepare the next set of inputs for the next generation step
        next_inputs = {
            "input_ids": next_token_ids.reshape((-1, 1)),  # Reshape the token IDs for batch input
            "position_ids": next_inputs["position_ids"][:, -1].unsqueeze(-1) + 1,  # Increment position IDs
            "attention_mask": torch.cat([  # Add attention mask for the new token
                next_inputs["attention_mask"],
                torch.ones((next_token_ids.shape[0], 1)),  # Add 1 for the new token in the attention mask
            ], dim=1),
            "past_key_values": past_key_values,  # Include past key-values for cache
        }

        # Decode the generated token IDs back to text
        next_tokens = tokenizer.batch_decode(next_token_ids)
        for i, token in enumerate(next_tokens):
            # Append the decoded token to the corresponding list in generated_tokens
            generated_tokens[i].append(token)

    # Return the final generated text as a list of strings, one for each input in the batch
    return ["".join(tokens) for tokens in generated_tokens]

# Set random seed for reproducibility
random.seed(42)

# Initialize parameters for batch processing
queue_size = 32
batch_size = 8

# Create a request queue with prompts and maximum token length for each request
request_queue = [
    (prompts[0], 100 if i % batch_size == 0 else 10)  # Assign a longer token length for the first item
    for i in range(queue_size)
]

# Split the request queue into batches of size `batch_size`
batches = [
    request_queue[i : i + batch_size]
    for i in range(0, len(request_queue), batch_size)
]

# Process each batch
for i, batch in enumerate(batches):
    # Determine the maximum number of tokens to generate in this batch
    batch_max_tokens = [b[1] for b in batch]
    max_tokens = max(batch_max_tokens)

    # Extract the prompts from the batch
    batch_prompts = [b[0] for b in batch]
    # Tokenize the prompts in the batch
    inputs = tokenizer(
        batch_prompts, padding=True, return_tensors="pt"
    )
    
    # Generate tokens for the batch with the determined maximum number of tokens
    generate_batch(inputs, max_tokens=max_tokens)

# Initialize a batch using the first `batch_size` items from the request queue
batch = init_batch(request_queue[:batch_size])
# Generate the next token for this batch
cached_batch = generate_next_token(batch)
# Update the request queue to remove the processed items
request_queue = request_queue[batch_size:]

# Continue generating tokens for subsequent batches while request_queue has items or cached_batch has remaining tokens
while (len(request_queue) > 0 or cached_batch["input_ids"].size(0) > 0):
    # Calculate the remaining capacity in the batch
    batch_capacity = batch_size - cached_batch["input_ids"].size(0)

    # If there is capacity left and the request queue has items, process the next batch
    if batch_capacity > 0 and len(request_queue) > 0:
        # Initialize a new batch from the request queue
        new_batch = init_batch(request_queue[:batch_capacity])
        # Generate the next token for the new batch
        new_batch = generate_next_token(new_batch)
        # Update the request queue to remove the processed items
        request_queue = request_queue[batch_capacity:]

        # Merge the new batch with the cached batch
        cached_batch = merge_batches(cached_batch, new_batch)
    
    # Generate the next token using the cached batch
    cached_batch = generate_next_token(cached_batch)

    # Filter out the completed sequences from the cached batch
    cached_batch, removed_indices = filter_batch(cached_batch)
